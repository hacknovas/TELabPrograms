{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "723eb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8b6b982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sachin', 'is', 'considered', 'to', 'be', 'one', 'of', 'the', 'greatest', 'cricket', 'players', '.', 'Virat', 'is', 'the', 'captain', 'of', 'the', 'Indian', 'cricket', 'team']\n",
      "['Sachin is considered to be one of the greatest cricket players.', 'Virat is the captain of the Indian cricket team']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "sent = \"Sachin is considered to be one of the greatest cricket players. Virat is the captain of the Indian cricket team\"\n",
    "print(word_tokenize(sent))\n",
    "print(sent_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "308116bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#import nltk\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8411fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the unclean version :  ['Sachin', 'is', 'considered', 'to', 'be', 'one', 'of', 'the', 'greatest', 'cricket', 'players', '.', 'Virat', 'is', 'the', 'captain', 'of', 'the', 'Indian', 'cricket', 'team']\n",
      "This is the cleaned version :  ['Sachin', 'considered', 'one', 'greatest', 'cricket', 'players', '.', 'Virat', 'captain', 'Indian', 'cricket', 'team']\n"
     ]
    }
   ],
   "source": [
    "token = word_tokenize(sent)\n",
    "cleaned_token = []\n",
    "for word in token:\n",
    " if word not in stop_words:\n",
    "    cleaned_token.append(word)\n",
    "\n",
    "print(\"This is the unclean version : \",token)\n",
    "print(\"This is the cleaned version : \",cleaned_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57060168",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [cleaned_token.lower() for cleaned_token in cleaned_token if cleaned_token.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a59215e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sachin', 'considered', 'one', 'greatest', 'cricket', 'players', 'virat', 'captain', 'indian', 'cricket', 'team']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d682e8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sachin', 'consid', 'one', 'greatest', 'cricket', 'player', 'virat', 'captain', 'indian', 'cricket', 'team']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "port_stemmer_output = [stemmer.stem(words) for words in words]\n",
    "print(port_stemmer_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c31f238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sachin', 'considered', 'one', 'greatest', 'cricket', 'player', 'virat', 'captain', 'indian', 'cricket', 'team']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer_output = [lemmatizer.lemmatize(words) for words in words]\n",
    "print(lemmatizer_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f6bede85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sachin', 'NN'), ('considered', 'VBD'), ('one', 'CD'), ('greatest', 'JJS'), ('cricket', 'NN'), ('player', 'NN'), ('virat', 'NN'), ('captain', 'NN'), ('indian', 'JJ'), ('cricket', 'NN'), ('team', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "#token = word_tokenize(sent)\n",
    "#cleaned_token = []\n",
    "#for word in token:\n",
    "# if word not in stop_words:\n",
    "#    cleaned_token.append(word)\n",
    "tagged = pos_tag(lemmatizer_output)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05fe5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c094e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c42ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab02b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['the cat see the mouse',\n",
    "      'the house has a tiny little mouse',\n",
    "       'the mouse ran away from the house',\n",
    "        'the cat finally ate the mouse',\n",
    "       'the end of the mouse story'\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46a730c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ate  away  cat  end  finally  from  has  house  little  mouse  of  ran   \n",
      "0    0     0    1    0        0     0    0      0       0      1   0    0  \\\n",
      "1    0     0    0    0        0     0    1      1       1      1   0    0   \n",
      "2    0     1    0    0        0     1    0      1       0      1   0    1   \n",
      "3    1     0    1    0        1     0    0      0       0      1   0    0   \n",
      "4    0     0    0    1        0     0    0      0       0      1   1    0   \n",
      "\n",
      "   see  story  the  tiny  \n",
      "0    1      0    2     0  \n",
      "1    0      0    1     1  \n",
      "2    0      0    2     0  \n",
      "3    0      0    2     0  \n",
      "4    0      1    2     0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\t\n",
    "cv = CountVectorizer()\n",
    "word_count_vector = cv.fit_transform(docs)\n",
    "tf = pd.DataFrame(word_count_vector.toarray(), columns=cv.get_feature_names())\n",
    "\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db2ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73b03857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_name  idf_weights\n",
      "0           ate     2.098612\n",
      "1          away     2.098612\n",
      "2           cat     1.693147\n",
      "3           end     2.098612\n",
      "4       finally     2.098612\n",
      "5          from     2.098612\n",
      "6           has     2.098612\n",
      "7         house     1.693147\n",
      "8        little     2.098612\n",
      "9         mouse     1.000000\n",
      "10           of     2.098612\n",
      "11          ran     2.098612\n",
      "12          see     2.098612\n",
      "13        story     2.098612\n",
      "14          the     1.000000\n",
      "15         tiny     2.098612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "X = tfidf_transformer.fit_transform(word_count_vector)\n",
    "\n",
    "idf = pd.DataFrame({'feature_name':cv.get_feature_names(), 'idf_weights':tfidf_transformer.idf_})\n",
    "\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbcdc95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6defc279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         be   captain  considered   cricket   federer  greatest    indian   \n",
      "0  0.405948  0.000000    0.259111  0.320054  0.000000  0.259111  0.000000  \\\n",
      "1  0.000000  0.000000    0.316458  0.000000  0.495792  0.316458  0.000000   \n",
      "2  0.000000  0.000000    0.316458  0.000000  0.000000  0.316458  0.000000   \n",
      "3  0.000000  0.399824    0.000000  0.315226  0.000000  0.000000  0.399824   \n",
      "\n",
      "         is     nadal        of       one   players    sachin      team   \n",
      "0  0.211841  0.000000  0.211841  0.259111  0.259111  0.405948  0.000000  \\\n",
      "1  0.258725  0.000000  0.258725  0.316458  0.316458  0.000000  0.000000   \n",
      "2  0.258725  0.495792  0.258725  0.316458  0.316458  0.000000  0.000000   \n",
      "3  0.208645  0.000000  0.208645  0.000000  0.000000  0.000000  0.399824   \n",
      "\n",
      "     tennis       the        to     virat  \n",
      "0  0.000000  0.211841  0.405948  0.000000  \n",
      "1  0.390888  0.258725  0.000000  0.000000  \n",
      "2  0.390888  0.258725  0.000000  0.000000  \n",
      "3  0.000000  0.417289  0.000000  0.399824  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tf_idf = pd.DataFrame(X.toarray() ,columns=cv.get_feature_names())\n",
    "print(tf_idf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
